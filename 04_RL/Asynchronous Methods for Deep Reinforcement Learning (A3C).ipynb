{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asynchronous Methods for Deep Reinforcement Learning (A3C).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4BBOlijrPPb9M1tyUb1c0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/respect5716/Deep-Learning-Paper-Implementation/blob/master/04_RL/Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%20(A3C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC",
        "colab_type": "text"
      },
      "source": [
        "# Asynchronous Methods for Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS",
        "colab_type": "text"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM",
        "colab_type": "text"
      },
      "source": [
        "### Info\n",
        "* TItle : Asynchronous Methods for Deep Reinforcement Learning\n",
        "* Author : Volodymyr Mnih et al.\n",
        "* Publication : ICML 2016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvL7j-dcEHBW",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "* asynchronous execution을 통해 데이터의 correlation 감소\n",
        "* 단일 CPU 만으로 빠른 학습 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTR7QXvpDz_j",
        "colab_type": "text"
      },
      "source": [
        "### Differences\n",
        "* Environment : Atari -> Cartpole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HVy-eYPLlUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import threading\n",
        "import multiprocessing\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw-eONQNEZIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU Setting\n",
        "!nvidia-smi\n",
        "\n",
        "print(f'tensorflow version : {tf.__version__}')\n",
        "print(f'available GPU list : {tf.config.list_physical_devices(\"GPU\")}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'base_dir' : '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "    'num_workers' : 16,\n",
        "    'n_step' : 5,\n",
        "    'gamma' : 0.95,\n",
        "    'beta' : 0.01,\n",
        "    'actor_lr' : 0.0001,\n",
        "    'critic_lr' : 0.0001,\n",
        "    'episode_size' : 5000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdnQjRJFn-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u06rqi1AF2f",
        "colab_type": "code",
        "outputId": "0dde1f2b-963a-472f-9e47-d0652a6a9684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "state.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHrbRvFLa41A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tracker(object):\n",
        "    def __init__(self):\n",
        "        self.episode_count = 0\n",
        "        self.score = 0\n",
        "    \n",
        "    def update(self, score):\n",
        "        self.episode_count += 1\n",
        "        self.score = CONFIG['gamma']*self.score + (1-CONFIG['gamma'])*score\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "        \n",
        "        \n",
        "    def store(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        \n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "def clip_gradient(g):\n",
        "    g, _ = tf.clip_by_global_norm(g, 40)\n",
        "    return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GlobalAgent(object):\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.actor_network, self.critic_network = self.build_network()\n",
        "        self.tracker = Tracker()\n",
        "    \n",
        "    def build_network(self):\n",
        "        inputs = tf.keras.layers.Input(self.state_space)\n",
        "        x = tf.keras.layers.Dense(100, activation='relu')(inputs)\n",
        "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
        "        actor_outputs = tf.keras.layers.Dense(self.action_space, activation='softmax')(x)\n",
        "        critic_outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "        \n",
        "        actor_network = tf.keras.Model(inputs, actor_outputs)\n",
        "        critic_network = tf.keras.Model(inputs, critic_outputs)\n",
        "        return actor_network, critic_network\n",
        "    \n",
        "    def train(self):\n",
        "        workers = [WorkerAgent(self.action_space, self.actor_network, self.critic_network, self.tracker) for _ in range(CONFIG['num_workers'])]\n",
        "        for worker in workers:\n",
        "            worker.start()\n",
        "        \n",
        "        for worker in workers:\n",
        "            worker.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fefrEvQZ808Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WorkerAgent(threading.Thread):\n",
        "    def __init__(self, action_space, actor_network, critic_network, tracker):\n",
        "        super(WorkerAgent, self).__init__()\n",
        "        self.env = env = gym.make('CartPole-v1')\n",
        "        self.action_space = action_space\n",
        "        self.actor_network = actor_network\n",
        "        self.critic_network = critic_network\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['actor_lr'])\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['critic_lr'])\n",
        "        self.worker_actor_network = tf.keras.models.clone_model(self.actor_network)\n",
        "        self.worker_critic_network = tf.keras.models.clone_model(self.critic_network)\n",
        "        self.tracker = tracker\n",
        "    \n",
        "    def act(self, state):\n",
        "        logits = self.worker_actor_network(state[None,:])\n",
        "        action = tf.random.categorical(logits, 1)[0][0].numpy()\n",
        "        return action\n",
        "    \n",
        "    def get_n_step_td_targets(self, rewards, next_v_value, done):\n",
        "        td_targets = np.zeros(len(rewards))\n",
        "        if done:\n",
        "            cumulative = 0\n",
        "        else:\n",
        "            cumulative = next_v_value\n",
        "\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            cumulative = rewards[i] + CONFIG['gamma'] * cumulative\n",
        "            td_targets[i] = cumulative\n",
        "        return td_targets\n",
        "\n",
        "    def train_actor_network(self, states, actions, advantages):\n",
        "        with tf.GradientTape() as g:\n",
        "            policy = self.actor_network(states)\n",
        "            policy = tf.reduce_sum(policy * tf.one_hot(actions, self.action_space), axis=-1)\n",
        "            log_policy = tf.math.log(policy + 1e-20)\n",
        "            entropy = -tf.reduce_sum(policy * log_policy)\n",
        "            loss = -(tf.reduce_sum(log_policy * advantages) + CONFIG['beta'] * entropy)\n",
        "\n",
        "        \n",
        "        gradients = g.gradient(loss, self.actor_network.trainable_variables)\n",
        "        gradients = clip_gradient(gradients)\n",
        "        self.actor_optimizer.apply_gradients(zip(gradients, self.actor_network.trainable_variables))\n",
        "    \n",
        "    def train_critic_network(self, states, td_targets):\n",
        "        with tf.GradientTape() as g:\n",
        "            v_values = tf.squeeze(self.critic_network(states))\n",
        "            loss = tf.reduce_mean(tf.square(td_targets - v_values))\n",
        "        \n",
        "        gradients = g.gradient(loss, self.critic_network.trainable_variables)\n",
        "        gradients = clip_gradient(gradients)\n",
        "        self.critic_optimizer.apply_gradients(zip(gradients, self.critic_network.trainable_variables))\n",
        "\n",
        "    def train(self, memory, next_state, done):\n",
        "        states = np.stack(memory.states)\n",
        "        actions = np.array(memory.actions)\n",
        "        rewards = np.array(memory.rewards)\n",
        "\n",
        "        v_values = self.worker_critic_network.predict(states)\n",
        "        next_v_value = self.worker_critic_network.predict(next_state[None,:])\n",
        "        n_step_td_targets = self.get_n_step_td_targets(rewards, next_v_value, done)\n",
        "        advantages = n_step_td_targets - v_values\n",
        "\n",
        "        self.train_actor_network(states, actions, advantages)\n",
        "        self.train_critic_network(states, n_step_td_targets)\n",
        "        self.worker_actor_network.set_weights(self.actor_network.get_weights())\n",
        "        self.worker_critic_network.set_weights(self.critic_network.get_weights())\n",
        "    \n",
        "    def run(self):\n",
        "        memory = Memory()\n",
        "        while self.tracker.episode_count < CONFIG['episode_size']:\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                score += reward\n",
        "                if done:\n",
        "                    reward = -1\n",
        "                memory.store(state, action, reward)\n",
        "\n",
        "                if len(memory) == CONFIG['n_step'] or done:\n",
        "                    self.train(memory, next_state, done)\n",
        "                    memory.clear()\n",
        "                \n",
        "                state = next_state\n",
        "            \n",
        "            if self.tracker.episode_count % 100 == 0:\n",
        "                ep = self.tracker.episode_count\n",
        "                sc = self.tracker.score\n",
        "                print(f'EP : {str(int(ep)).zfill(5)} | Score : {int(sc)}')\n",
        "\n",
        "            self.tracker.update(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhyQw7zyFq8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = GlobalAgent((4,), 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faD7ENUUxy7V",
        "colab_type": "code",
        "outputId": "8a19f6e8-d5c2-4a63-dc6d-320b13d13cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP : 00000 | Score : 0\n",
            "EP : 00100 | Score : 23\n",
            "EP : 00200 | Score : 32\n",
            "EP : 00300 | Score : 38\n",
            "EP : 00400 | Score : 32\n",
            "EP : 00500 | Score : 40\n",
            "EP : 00600 | Score : 25\n",
            "EP : 00700 | Score : 23\n",
            "EP : 00800 | Score : 42\n",
            "EP : 00900 | Score : 54\n",
            "EP : 01000 | Score : 59\n",
            "EP : 01100 | Score : 67\n",
            "EP : 01200 | Score : 76\n",
            "EP : 01300 | Score : 75\n",
            "EP : 01400 | Score : 89\n",
            "EP : 01500 | Score : 92\n",
            "EP : 01600 | Score : 112\n",
            "EP : 01700 | Score : 86\n",
            "EP : 01800 | Score : 92\n",
            "EP : 01900 | Score : 115\n",
            "EP : 02000 | Score : 107\n",
            "EP : 02100 | Score : 97\n",
            "EP : 02200 | Score : 101\n",
            "EP : 02300 | Score : 122\n",
            "EP : 02400 | Score : 128\n",
            "EP : 02500 | Score : 132\n",
            "EP : 02600 | Score : 119\n",
            "EP : 02700 | Score : 125\n",
            "EP : 02800 | Score : 161\n",
            "EP : 02900 | Score : 129\n",
            "EP : 03000 | Score : 149\n",
            "EP : 03100 | Score : 89\n",
            "EP : 03200 | Score : 100\n",
            "EP : 03300 | Score : 107\n",
            "EP : 03400 | Score : 120\n",
            "EP : 03500 | Score : 153\n",
            "EP : 03600 | Score : 131\n",
            "EP : 03700 | Score : 117\n",
            "EP : 03800 | Score : 119\n",
            "EP : 03900 | Score : 133\n",
            "EP : 04000 | Score : 140\n",
            "EP : 04100 | Score : 117\n",
            "EP : 04200 | Score : 123\n",
            "EP : 04300 | Score : 151\n",
            "EP : 04400 | Score : 113\n",
            "EP : 04500 | Score : 137\n",
            "EP : 04600 | Score : 134\n",
            "EP : 04700 | Score : 155\n",
            "EP : 04800 | Score : 111\n",
            "EP : 04900 | Score : 161\n",
            "EP : 05000 | Score : 143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJetCrTFr_S",
        "colab_type": "text"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3HN95nRFs-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "dbb648d8-071e-4fc7-83b8-f8c6895746d8"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "for i in range(5):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        policy = agent.actor_network(state[None,:])\n",
        "        action = np.argmax(policy[0])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        state = next_state\n",
        "    \n",
        "    print(f'EP : {i+1} | Score : {int(score)}')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP : 1 | Score : 313\n",
            "EP : 2 | Score : 262\n",
            "EP : 3 | Score : 337\n",
            "EP : 4 | Score : 275\n",
            "EP : 5 | Score : 349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MszVjdHYqc-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}