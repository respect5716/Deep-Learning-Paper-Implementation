{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proximal Policy Optimization Algorithms (PPO).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFQpDJ7ZFzqLArXRaRm1pz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/respect5716/Deep-Learning-Paper-Implementation/blob/master/04_RL/Proximal%20Policy%20Optimization%20Algorithms%20(PPO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC",
        "colab_type": "text"
      },
      "source": [
        "# Proximal Policy Optimization Algorithms (PPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS",
        "colab_type": "text"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM",
        "colab_type": "text"
      },
      "source": [
        "### Info\n",
        "* TItle : Proximal Policy Optimization Algorithms (PPO)\n",
        "* Author : John Schulman et al.\n",
        "* Publication : [link](https://arxiv.org/pdf/1707.06347.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvL7j-dcEHBW",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "* Clipped surrogate objective function을 통해 데이터를 여러번 학습할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTR7QXvpDz_j",
        "colab_type": "text"
      },
      "source": [
        "### Differences\n",
        "* Environment : Atari -> Cartpole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HVy-eYPLlUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw-eONQNEZIl",
        "colab_type": "code",
        "outputId": "b07123b3-fc31-47d5-bf13-654afe16b93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# GPU Setting\n",
        "!nvidia-smi\n",
        "\n",
        "print(f'tensorflow version : {tf.__version__}')\n",
        "print(f'available GPU list : {tf.config.list_physical_devices(\"GPU\")}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  2 06:22:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "tensorflow version : 2.2.0\n",
            "available GPU list : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'base_dir' : '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "    'gamma' : 0.99, # discount rate\n",
        "    'lambda' : 0.95, # gae parameter\n",
        "    'epsilon' : 0.2, # clipping parameter\n",
        "    'c1' : 1, # critic loss coefficient\n",
        "    'c2' : 0.01, # entropy bonus coefficient,\n",
        "    'len_memory' : 128,\n",
        "    'learning_rate' : 1e-3,\n",
        "    'batch_size' : 32,\n",
        "    'epoch_size' : 3,\n",
        "    'episode_size' : 3000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdnQjRJFn-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_uYQGdDfy5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "    \n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.next_states.append(next_state)\n",
        "        self.dones.append(done)\n",
        "        \n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.next_states = []\n",
        "        self.dones = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.network = self.build_network()\n",
        "        self.network_old = tf.keras.models.clone_model(self.network)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(CONFIG['learning_rate'])\n",
        "        self.step_size = 0\n",
        "\n",
        "    def build_network(self):\n",
        "        inputs = tf.keras.layers.Input(self.state_space)\n",
        "        x = tf.keras.layers.Dense(100, activation='relu')(inputs)\n",
        "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
        "        actor_outputs = tf.keras.layers.Dense(self.action_space, activation='softmax')(x)\n",
        "        critic_outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "        \n",
        "        network = tf.keras.Model(inputs, [actor_outputs, critic_outputs])\n",
        "        return network\n",
        "    \n",
        "    def act(self, state):\n",
        "        policy, _ = self.network(state[None,:])\n",
        "        action = tf.random.categorical(policy, 1)[0][0].numpy()\n",
        "        return action\n",
        "    \n",
        "    def get_gae_and_td(self, rewards, dones, values, next_values):\n",
        "        gae = np.zeros(len(rewards))\n",
        "        td_targets = np.zeros(len(rewards))\n",
        "        gae_cumulative = 0\n",
        "\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            if dones[i]:\n",
        "                gae_cumulative = 0\n",
        "            td_targets[i] = rewards[i] + (1-dones[i])*CONFIG['gamma']*next_values[i]\n",
        "            delta = td_targets[i] - values[i]    \n",
        "            gae_cumulative = delta + CONFIG['gamma']*CONFIG['lambda']*gae_cumulative\n",
        "            gae[i] = gae_cumulative\n",
        "        return gae, td_targets\n",
        "\n",
        "    def train(self, memory):\n",
        "        states = np.stack(memory.states)\n",
        "        actions = np.array(memory.actions)\n",
        "        rewards = np.array(memory.rewards)\n",
        "        next_states = np.array(memory.next_states)\n",
        "        dones = np.array(memory.dones)\n",
        "\n",
        "        _, values = self.network(states)\n",
        "        _, next_values = self.network(next_states)\n",
        "\n",
        "        values = values[:,0]\n",
        "        next_values = next_values[:,0]\n",
        "        gae, td_targets = self.get_gae_and_td(rewards, dones, values, next_values)\n",
        "\n",
        "        for i in range(CONFIG['epoch_size']):\n",
        "            batch_step_size = CONFIG['len_memory'] // CONFIG['batch_size']\n",
        "            for _ in range(batch_step_size):\n",
        "                idx = np.random.randint(0, CONFIG['len_memory'], CONFIG['batch_size'])\n",
        "                batch_states = states[idx]\n",
        "                batch_actions = actions[idx]\n",
        "                batch_gae = gae[idx]\n",
        "                batch_td_targets = td_targets[idx]\n",
        "\n",
        "                self.train_step(batch_states, batch_actions, batch_gae, batch_td_targets)\n",
        "                self.step_size += 1\n",
        "        self.network_old.set_weights(self.network.get_weights())\n",
        "\n",
        "    def train_step(self, states, actions, gae_targets, td_targets):\n",
        "        with tf.GradientTape() as g:\n",
        "            prob, values = self.network(states)\n",
        "            prob_old, _ = self.network_old(states)\n",
        "            policy = tf.reduce_sum(prob * tf.one_hot(actions, self.action_space), axis=1)\n",
        "            policy_old = tf.reduce_sum(prob_old * tf.one_hot(actions, self.action_space), axis=1)\n",
        "\n",
        "            log_policy = tf.math.log(policy + 1e-20)\n",
        "            log_policy_old = tf.math.log(policy_old + 1e-20)\n",
        "\n",
        "            rt = tf.math.exp(log_policy - log_policy_old)\n",
        "            clipped_rt = tf.clip_by_value(rt, 1-CONFIG['epsilon'], 1+CONFIG['epsilon'])\n",
        "            \n",
        "            actor_loss = tf.reduce_mean(tf.minimum(rt*gae_targets, clipped_rt*gae_targets))\n",
        "            critic_loss = tf.reduce_mean(tf.square(td_targets - values[:,0]))\n",
        "            entropy = tf.reduce_mean(-tf.reduce_sum(prob * tf.math.log(prob+1e-5), axis=1))\n",
        "            loss = tf.reduce_mean(actor_loss - CONFIG['c1']*critic_loss + CONFIG['c2']*entropy)\n",
        "            loss = -loss\n",
        "        \n",
        "        gradients = g.gradient(loss, self.network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhyQw7zyFq8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = Agent((4,), 2)\n",
        "memory = Memory()\n",
        "score = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpTqodKFrwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "27d49892-a89f-402f-f0f8-ba48a94c25e7"
      },
      "source": [
        "for ep in range(1, CONFIG['episode_size']+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_score = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode_score += reward\n",
        "\n",
        "        memory.store(state, action, reward, next_state, done)\n",
        "        if len(memory) >= CONFIG['len_memory']:\n",
        "            agent.train(memory)\n",
        "            memory.reset()\n",
        "        state = next_state\n",
        "\n",
        "    score = 0.9*score + 0.1*episode_score\n",
        "    if (ep % 100 == 0) or (ep == CONFIG['episode_size']):\n",
        "        print(f'EP : {str(ep).zfill(4)} | Score : {str(int(score)).zfill(3)} | Step size : {str(agent.step_size).zfill(5)}')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP : 0100 | Score : 056 | Step size : 00348\n",
            "EP : 0200 | Score : 086 | Step size : 01128\n",
            "EP : 0300 | Score : 142 | Step size : 02292\n",
            "EP : 0400 | Score : 127 | Step size : 03516\n",
            "EP : 0500 | Score : 109 | Step size : 04620\n",
            "EP : 0600 | Score : 106 | Step size : 05472\n",
            "EP : 0700 | Score : 157 | Step size : 06924\n",
            "EP : 0800 | Score : 126 | Step size : 08196\n",
            "EP : 0900 | Score : 156 | Step size : 09792\n",
            "EP : 1000 | Score : 108 | Step size : 11064\n",
            "EP : 1100 | Score : 145 | Step size : 12384\n",
            "EP : 1200 | Score : 104 | Step size : 13596\n",
            "EP : 1300 | Score : 125 | Step size : 14808\n",
            "EP : 1400 | Score : 119 | Step size : 16032\n",
            "EP : 1500 | Score : 105 | Step size : 17340\n",
            "EP : 1600 | Score : 131 | Step size : 18708\n",
            "EP : 1700 | Score : 155 | Step size : 20052\n",
            "EP : 1800 | Score : 132 | Step size : 21204\n",
            "EP : 1900 | Score : 160 | Step size : 22476\n",
            "EP : 2000 | Score : 140 | Step size : 23772\n",
            "EP : 2100 | Score : 178 | Step size : 25128\n",
            "EP : 2200 | Score : 136 | Step size : 26484\n",
            "EP : 2300 | Score : 119 | Step size : 27672\n",
            "EP : 2400 | Score : 139 | Step size : 29016\n",
            "EP : 2500 | Score : 124 | Step size : 30180\n",
            "EP : 2600 | Score : 096 | Step size : 31296\n",
            "EP : 2700 | Score : 132 | Step size : 32388\n",
            "EP : 2800 | Score : 110 | Step size : 33516\n",
            "EP : 2900 | Score : 144 | Step size : 34848\n",
            "EP : 3000 | Score : 130 | Step size : 36060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJetCrTFr_S",
        "colab_type": "text"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3HN95nRFs-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "53384f65-52f9-47f5-83e2-5c65e287f2ab"
      },
      "source": [
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        policy, _ = agent.network(state[None,:])\n",
        "        action = tf.argmax(policy[0]).numpy()\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        state = next_state\n",
        "    \n",
        "    print(f'EP : {str(i)} | Score : {str(int(score)).zfill(3)}')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP : 0 | Score : 383\n",
            "EP : 1 | Score : 420\n",
            "EP : 2 | Score : 344\n",
            "EP : 3 | Score : 431\n",
            "EP : 4 | Score : 384\n",
            "EP : 5 | Score : 367\n",
            "EP : 6 | Score : 375\n",
            "EP : 7 | Score : 421\n",
            "EP : 8 | Score : 386\n",
            "EP : 9 | Score : 420\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}