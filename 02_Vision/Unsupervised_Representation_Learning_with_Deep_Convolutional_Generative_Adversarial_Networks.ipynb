{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC",
        "colab_type": "text"
      },
      "source": [
        "# Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS",
        "colab_type": "text"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM",
        "colab_type": "text"
      },
      "source": [
        "### Info\n",
        "* TItle : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
        "* Author : Alec Radford et al.\n",
        "* Link : https://arxiv.org/pdf/1511.06434.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvL7j-dcEHBW",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "* (strided) CNN, BN 등의 테크닉으로 GAN 학습 안정화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTR7QXvpDz_j",
        "colab_type": "text"
      },
      "source": [
        "### Features\n",
        "* dataset 변경 : LSUN -> Fashion mnist\n",
        "* 차원 수 줄임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'base_dir' : '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "    'batch_size' : 128,\n",
        "    'd_step_size' : 2,\n",
        "    'g_step_size' : 5,\n",
        "    'step_size' : 3000\n",
        "}"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQceVqIfj8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep_image(_images):\n",
        "    _images = _images.astype(np.float32)\n",
        "    images = np.zeros((_images.shape[0], 32, 32)).astype(np.float32) # padding\n",
        "    images[:, 2:30, 2:30] = _images\n",
        "    images = (2 * images - 255.0) / 255.0 # -1 ~ 1\n",
        "    images = images[:,:,:,None]\n",
        "    return images"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdnQjRJFn-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = prep_image(x_train)\n",
        "x_test = prep_image(x_test)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nZEs03rfh70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f5441ff-0e89-4fde-955f-0626d696e7b1"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 32, 32, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvgUtrvtOCCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = tf.keras.layers.Dense(2*2*128, activation='relu', kernel_initializer='random_normal')\n",
        "        self.conv1 = tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2DTranspose(32, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.keras.layers.Conv2DTranspose(16, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv4 = tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = tf.reshape(x, (-1, 2, 2, 128)) \n",
        "        x = tf.nn.relu(self.bn1(self.conv1(x)))\n",
        "        x = tf.nn.relu(self.bn2(self.conv2(x)))\n",
        "        x = tf.nn.relu(self.bn3(self.conv3(x)))\n",
        "        x = tf.nn.tanh(self.conv4(x))\n",
        "        return x\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv4 = tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same', kernel_initializer='random_normal')\n",
        "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x)))\n",
        "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x)))\n",
        "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh8Q_7H6RAnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = Generator()\n",
        "d = Discriminator()"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcORx5HlXTQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def d_step(real, fake):\n",
        "    with tf.GradientTape() as tape:\n",
        "        real_loss = -tf.math.log(d(real) + 1e-7)\n",
        "        fake_loss = -tf.math.log(1 - d(fake) + 1e-7)\n",
        "        loss = real_loss + fake_loss\n",
        "    gradients = tape.gradient(loss, d.trainable_variables)\n",
        "    d.optimizer.apply_gradients(zip(gradients, d.trainable_variables))\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "@tf.function\n",
        "def g_step(z):\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake = g(z)\n",
        "        loss = - tf.math.log(d(fake) + 1e-7)\n",
        "    gradients = tape.gradient(loss, g.trainable_variables)\n",
        "    g.optimizer.apply_gradients(zip(gradients, g.trainable_variables))\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def post_image(images):\n",
        "    images = images.numpy()\n",
        "    images = (255 * images + 255) / 2\n",
        "    images = images.astype(np.int32)\n",
        "    images = images[:,:,:,0]\n",
        "    return images "
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seLtkwSJX4B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for st in range(CONFIG['step_size']):\n",
        "\n",
        "    for _ in range(CONFIG['d_step_size']):\n",
        "        z = np.random.normal(size=(CONFIG['batch_size'], 1000)).astype(np.float32)\n",
        "        fake = g(z)\n",
        "        real = x_train[np.random.permutation(len(x_train))[:CONFIG['batch_size']]]\n",
        "        d_loss = d_step(real, fake)\n",
        "\n",
        "    for _ in range(CONFIG['g_step_size']):\n",
        "        z = np.random.normal(size=(CONFIG['batch_size'], 1000)).astype(np.float32)\n",
        "        g_loss = g_step(z)\n",
        "\n",
        "    if st % 100 == 0:\n",
        "        print(f'STEP : {str(st).zfill(5)} | D Loss : {d_loss.numpy():.3f} | G Loss : {g_loss.numpy():.3f}')\n",
        "        fake = post_image(fake)\n",
        "        plt.imshow(fake[0])\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJetCrTFr_S",
        "colab_type": "text"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3HN95nRFs-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = np.random.normal(size=(CONFIG['batch_size'], 1000)).astype(np.float32)\n",
        "images = g(z)\n",
        "images = post_image(images)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qchwTR7AZmgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "f9dd27bc-dd88-4a25-a2cc-e69be0764709"
      },
      "source": [
        "plt.imshow(images[0])\n",
        "plt.show()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}