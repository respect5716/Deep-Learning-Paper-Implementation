{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An Image Is Worth 16x16 Words _ Transformers for Image Recognition at Scale",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC"
      },
      "source": [
        "# An Image Is Worth 16x16 Words _ Transformers for Image Recognition at Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM"
      },
      "source": [
        "### Info\n",
        "* TItle : An Image Is Worth 16x16 Words _ Transformers for Image Recognition at Scale\n",
        "* Author : Alexey Dosovitskiy\n",
        "* Link : https://arxiv.org/pdf/2010.11929.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvL7j-dcEHBW"
      },
      "source": [
        "### Summary\n",
        "* 이미지를 여러 patch로 분리해 token처럼 사용\n",
        "* transformer 모델을 통해 image recognition 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTR7QXvpDz_j"
      },
      "source": [
        "### Features\n",
        "* dataset : cifar10\n",
        "* test acc가 0.5 수준에서 멈춰있다. 구현 상의 문제는 없는 것 같은데, 정확히 어떤 문제인지 알 수 없다. 논문에서 작은 이미지에서 학습이 제대로 안 된다고 언급한 부분에 해당하는 걸까"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L"
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g"
      },
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'base_dir' : '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "    'drop_rate' : 0.1,\n",
        "    'num_head' : 4,\n",
        "    'num_layer' : 16,\n",
        "    'model_dim' : 512,\n",
        "    'patch_size' : 8,\n",
        "    'batch_size' : 256,\n",
        "    'epoch_size' : 100\n",
        "}"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIoP7VcNuE-r"
      },
      "source": [
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x, y, mode):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.mode = mode\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return np.ceil(len(self.x) / CONFIG['batch_size']).astype(np.int32)\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.mode == 'train':\n",
        "            self.indices = np.random.permutation(len(self.x))\n",
        "        else:\n",
        "            self.indices = np.arange(len(self.x))\n",
        "    \n",
        "    def make_patch(self, x):\n",
        "        patch = []\n",
        "        batch_size = x.shape[0]\n",
        "        seq_len = x.shape[1] // CONFIG['patch_size']\n",
        "        for i in range(seq_len):\n",
        "            _patch = x[:, CONFIG['patch_size']*i:CONFIG['patch_size']*(i+1), CONFIG['patch_size']*i:CONFIG['patch_size']*(i+1), :]\n",
        "            _patch = np.reshape(_patch, (batch_size, -1))\n",
        "            patch.append(_patch)\n",
        "        patch = np.stack(patch, axis=1)\n",
        "        return patch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indices[CONFIG['batch_size'] * idx : CONFIG['batch_size'] * (idx+1)]\n",
        "        batch_x = self.x[batch_idx]\n",
        "        batch_x = self.make_patch(batch_x)\n",
        "        batch_x = batch_x.astype(np.float32) / 255.0\n",
        "        batch_y = self.y[batch_idx][:,0]\n",
        "        return batch_x, batch_y"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdnQjRJFn-M"
      },
      "source": [
        "train_loader = Dataloader(x_train, y_train, 'train')\n",
        "test_loader = Dataloader(x_test, y_test, 'test')"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49QLwU-3vW_U",
        "outputId": "1c4a9222-5df1-45be-c5ad-d018881a81fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x, y = train_loader.__getitem__(0)\n",
        "x.shape, y.shape"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((256, 4, 192), (256,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47"
      },
      "source": [
        "class EmbeddingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, seq_len):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.cls = tf.Variable(tf.random.normal((1, 1, model_dim)))\n",
        "        self.embedding = tf.keras.layers.Dense(model_dim)\n",
        "        self.pos = tf.Variable(tf.random.normal((1, seq_len+1, model_dim)))\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = self.embedding(x)\n",
        "        cls = tf.repeat(self.cls, batch_size, axis=0)\n",
        "        x = tf.concat([cls, x], axis=1)\n",
        "        x += self.pos\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, num_head, drop_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_head = num_head\n",
        "        self.projection_dim = self.model_dim // self.num_head\n",
        "        assert self.model_dim % self.num_head == 0\n",
        "\n",
        "        self.qw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.kw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.vw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.w = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "    \n",
        "    def attention(self, q, k ,v, mask):\n",
        "        dim = tf.cast(tf.shape(q)[-1], tf.float32)\n",
        "        score = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_score = score / tf.math.sqrt(dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_score += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_score)\n",
        "        attention_outputs = tf.matmul(attention_weights, v)\n",
        "        return attention_outputs, attention_weights\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_head, self.projection_dim))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return x\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        x = tf.reshape(x, (batch_size, -1, self.model_dim))\n",
        "        return x\n",
        "    \n",
        "    def call(self, q, k, v, mask=None):\n",
        "        q, k, v = self.qw(q), self.kw(k), self.vw(v)\n",
        "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "        outputs, weights = self.attention(q, k, v, mask)\n",
        "        outputs = self.combine_heads(outputs)\n",
        "        outputs = self.w(outputs)\n",
        "        outputs = self.dropout(outputs)\n",
        "        return outputs\n",
        "\n",
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, drop_rate):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(model_dim)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dense2 = tf.keras.layers.Dense(model_dim)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    @staticmethod\n",
        "    def gelu(x):\n",
        "        cdf = 0.5 * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n",
        "        return x * cdf\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, num_head, drop_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = MultiHeadAttention(model_dim, num_head, drop_rate)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn = FeedForwardNetwork(model_dim, drop_rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        inputs = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mha(x, x, x)\n",
        "        x += inputs\n",
        "\n",
        "        inputs = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x += inputs\n",
        "        return x\n",
        "\n",
        "class OutputLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_class):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dense = tf.keras.layers.Dense(num_class, activation='softmax')\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.norm(x[:, 0, :])\n",
        "        outputs = self.dense(x)\n",
        "        return outputs\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, seq_len,  num_class):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding = EmbeddingLayer(CONFIG['model_dim'], seq_len)\n",
        "        self.encoders = [EncoderLayer(CONFIG['model_dim'], CONFIG['num_head'], CONFIG['drop_rate']) for _ in range(CONFIG['num_layer'])]\n",
        "        self.output_layer = OutputLayer(num_class)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        x = self.embedding(x)\n",
        "        for i in range(CONFIG['num_layer']):\n",
        "            x = self.encoders[i](x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7_PUNrFpCS"
      },
      "source": [
        "model = Model(seq_len=4, num_class=10)\n",
        "\n",
        "model.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['acc'],\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        ")"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhyQw7zyFq8B",
        "outputId": "754887f6-3be2-43a6-ef1e-4c259c2cb5a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hist = model.fit(\n",
        "    train_loader,\n",
        "    validation_data = test_loader,\n",
        "    epochs = CONFIG['epoch_size']\n",
        ")"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "196/196 [==============================] - 19s 95ms/step - loss: 2.5637 - acc: 0.1030 - val_loss: 2.3104 - val_acc: 0.1351\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 2.1723 - acc: 0.1764 - val_loss: 2.0037 - val_acc: 0.2627\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.9555 - acc: 0.2856 - val_loss: 1.9298 - val_acc: 0.3047\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.9279 - acc: 0.2939 - val_loss: 1.8922 - val_acc: 0.3109\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.8701 - acc: 0.3242 - val_loss: 1.8320 - val_acc: 0.3400\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.8938 - acc: 0.3091 - val_loss: 1.8182 - val_acc: 0.3323\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.8199 - acc: 0.3425 - val_loss: 1.7315 - val_acc: 0.3769\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.7341 - acc: 0.3736 - val_loss: 1.6790 - val_acc: 0.3961\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.6946 - acc: 0.3874 - val_loss: 1.6948 - val_acc: 0.3887\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.6653 - acc: 0.4021 - val_loss: 1.6134 - val_acc: 0.4189\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.6281 - acc: 0.4140 - val_loss: 1.6115 - val_acc: 0.4168\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.6048 - acc: 0.4231 - val_loss: 1.6259 - val_acc: 0.4186\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.5783 - acc: 0.4345 - val_loss: 1.5531 - val_acc: 0.4411\n",
            "Epoch 14/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5514 - acc: 0.4435 - val_loss: 1.5741 - val_acc: 0.4366\n",
            "Epoch 15/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5478 - acc: 0.4449 - val_loss: 1.5806 - val_acc: 0.4367\n",
            "Epoch 16/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5206 - acc: 0.4546 - val_loss: 1.5200 - val_acc: 0.4584\n",
            "Epoch 17/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5073 - acc: 0.4597 - val_loss: 1.5244 - val_acc: 0.4546\n",
            "Epoch 18/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.6012 - acc: 0.4194 - val_loss: 1.5504 - val_acc: 0.4474\n",
            "Epoch 19/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5037 - acc: 0.4594 - val_loss: 1.5680 - val_acc: 0.4383\n",
            "Epoch 20/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.5294 - acc: 0.4497 - val_loss: 1.5247 - val_acc: 0.4464\n",
            "Epoch 21/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.4822 - acc: 0.4684 - val_loss: 1.4816 - val_acc: 0.4702\n",
            "Epoch 22/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.4696 - acc: 0.4717 - val_loss: 1.5163 - val_acc: 0.4541\n",
            "Epoch 23/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.4689 - acc: 0.4739 - val_loss: 1.4928 - val_acc: 0.4692\n",
            "Epoch 24/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.4252 - acc: 0.4898 - val_loss: 1.4751 - val_acc: 0.4716\n",
            "Epoch 25/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.4093 - acc: 0.4950 - val_loss: 1.5066 - val_acc: 0.4627\n",
            "Epoch 26/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.4350 - acc: 0.4852 - val_loss: 1.4842 - val_acc: 0.4713\n",
            "Epoch 27/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.3733 - acc: 0.5094 - val_loss: 1.4426 - val_acc: 0.4884\n",
            "Epoch 28/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.3594 - acc: 0.5147 - val_loss: 1.4531 - val_acc: 0.4806\n",
            "Epoch 29/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.3358 - acc: 0.5233 - val_loss: 1.4495 - val_acc: 0.4894\n",
            "Epoch 30/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.3077 - acc: 0.5331 - val_loss: 1.4306 - val_acc: 0.4892\n",
            "Epoch 31/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.2824 - acc: 0.5424 - val_loss: 1.4614 - val_acc: 0.4827\n",
            "Epoch 32/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.2685 - acc: 0.5489 - val_loss: 1.4595 - val_acc: 0.4907\n",
            "Epoch 33/100\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.2361 - acc: 0.5583 - val_loss: 1.4346 - val_acc: 0.4932\n",
            "Epoch 34/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.2135 - acc: 0.5657 - val_loss: 1.4299 - val_acc: 0.4973\n",
            "Epoch 35/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.1991 - acc: 0.5700 - val_loss: 1.4658 - val_acc: 0.4951\n",
            "Epoch 36/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.1722 - acc: 0.5788 - val_loss: 1.4795 - val_acc: 0.4770\n",
            "Epoch 37/100\n",
            "196/196 [==============================] - 15s 76ms/step - loss: 1.1424 - acc: 0.5898 - val_loss: 1.4922 - val_acc: 0.4854\n",
            "Epoch 38/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.0937 - acc: 0.6060 - val_loss: 1.5241 - val_acc: 0.4940\n",
            "Epoch 39/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.0540 - acc: 0.6193 - val_loss: 1.5408 - val_acc: 0.4899\n",
            "Epoch 40/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 1.0132 - acc: 0.6353 - val_loss: 1.5807 - val_acc: 0.4898\n",
            "Epoch 41/100\n",
            "196/196 [==============================] - 15s 76ms/step - loss: 0.9649 - acc: 0.6518 - val_loss: 1.6254 - val_acc: 0.4882\n",
            "Epoch 42/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.9149 - acc: 0.6682 - val_loss: 1.6211 - val_acc: 0.4877\n",
            "Epoch 43/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.9157 - acc: 0.6686 - val_loss: 1.6070 - val_acc: 0.4933\n",
            "Epoch 44/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.8264 - acc: 0.7025 - val_loss: 1.7447 - val_acc: 0.4833\n",
            "Epoch 45/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.7665 - acc: 0.7199 - val_loss: 1.8004 - val_acc: 0.4800\n",
            "Epoch 46/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.7332 - acc: 0.7344 - val_loss: 1.8318 - val_acc: 0.4804\n",
            "Epoch 47/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.6620 - acc: 0.7600 - val_loss: 1.9486 - val_acc: 0.4725\n",
            "Epoch 48/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.6141 - acc: 0.7742 - val_loss: 2.0896 - val_acc: 0.4710\n",
            "Epoch 49/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.5882 - acc: 0.7857 - val_loss: 2.1805 - val_acc: 0.4739\n",
            "Epoch 50/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.5262 - acc: 0.8073 - val_loss: 2.2263 - val_acc: 0.4769\n",
            "Epoch 51/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4762 - acc: 0.8274 - val_loss: 2.2977 - val_acc: 0.4682\n",
            "Epoch 52/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4358 - acc: 0.8397 - val_loss: 2.4671 - val_acc: 0.4715\n",
            "Epoch 53/100\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 0.4009 - acc: 0.8524 - val_loss: 2.5921 - val_acc: 0.4683\n",
            "Epoch 54/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4155 - acc: 0.8478 - val_loss: 2.4769 - val_acc: 0.4711\n",
            "Epoch 55/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4848 - acc: 0.8229 - val_loss: 2.0788 - val_acc: 0.4694\n",
            "Epoch 56/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4777 - acc: 0.8254 - val_loss: 2.1231 - val_acc: 0.4860\n",
            "Epoch 57/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4474 - acc: 0.8375 - val_loss: 2.3145 - val_acc: 0.4717\n",
            "Epoch 58/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4346 - acc: 0.8418 - val_loss: 2.2092 - val_acc: 0.4842\n",
            "Epoch 59/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.4156 - acc: 0.8485 - val_loss: 2.3362 - val_acc: 0.4792\n",
            "Epoch 60/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.3883 - acc: 0.8586 - val_loss: 2.3992 - val_acc: 0.4776\n",
            "Epoch 61/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.3842 - acc: 0.8629 - val_loss: 2.4331 - val_acc: 0.4824\n",
            "Epoch 62/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.3425 - acc: 0.8747 - val_loss: 2.4522 - val_acc: 0.4804\n",
            "Epoch 63/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.3407 - acc: 0.8758 - val_loss: 2.5053 - val_acc: 0.4755\n",
            "Epoch 64/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.3281 - acc: 0.8800 - val_loss: 2.5764 - val_acc: 0.4743\n",
            "Epoch 65/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.3459 - acc: 0.8749 - val_loss: 2.4347 - val_acc: 0.4682\n",
            "Epoch 66/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.3314 - acc: 0.8795 - val_loss: 2.5622 - val_acc: 0.4750\n",
            "Epoch 67/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2776 - acc: 0.9012 - val_loss: 2.7762 - val_acc: 0.4810\n",
            "Epoch 68/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2636 - acc: 0.9042 - val_loss: 2.7008 - val_acc: 0.4784\n",
            "Epoch 69/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2538 - acc: 0.9087 - val_loss: 2.7903 - val_acc: 0.4798\n",
            "Epoch 70/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.2475 - acc: 0.9110 - val_loss: 2.8082 - val_acc: 0.4836\n",
            "Epoch 71/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.2380 - acc: 0.9155 - val_loss: 2.8207 - val_acc: 0.4765\n",
            "Epoch 72/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.2402 - acc: 0.9136 - val_loss: 2.8272 - val_acc: 0.4716\n",
            "Epoch 73/100\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.2298 - acc: 0.9171 - val_loss: 2.8694 - val_acc: 0.4810\n",
            "Epoch 74/100\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 0.2195 - acc: 0.9214 - val_loss: 2.9208 - val_acc: 0.4776\n",
            "Epoch 75/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.2243 - acc: 0.9198 - val_loss: 3.2831 - val_acc: 0.4544\n",
            "Epoch 76/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1739 - acc: 0.9388 - val_loss: 3.4178 - val_acc: 0.4785\n",
            "Epoch 77/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1171 - acc: 0.9588 - val_loss: 3.7708 - val_acc: 0.4675\n",
            "Epoch 78/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1104 - acc: 0.9615 - val_loss: 3.8693 - val_acc: 0.4721\n",
            "Epoch 79/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1779 - acc: 0.9377 - val_loss: 3.0912 - val_acc: 0.4768\n",
            "Epoch 80/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2133 - acc: 0.9241 - val_loss: 2.9272 - val_acc: 0.4810\n",
            "Epoch 81/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1626 - acc: 0.9434 - val_loss: 3.0346 - val_acc: 0.4799\n",
            "Epoch 82/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2408 - acc: 0.9167 - val_loss: 3.1952 - val_acc: 0.4748\n",
            "Epoch 83/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1483 - acc: 0.9479 - val_loss: 3.3665 - val_acc: 0.4769\n",
            "Epoch 84/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1051 - acc: 0.9630 - val_loss: 3.2608 - val_acc: 0.4699\n",
            "Epoch 85/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.2037 - acc: 0.9281 - val_loss: 2.9246 - val_acc: 0.4816\n",
            "Epoch 86/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1365 - acc: 0.9518 - val_loss: 3.5887 - val_acc: 0.4761\n",
            "Epoch 87/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0970 - acc: 0.9662 - val_loss: 3.6489 - val_acc: 0.4678\n",
            "Epoch 88/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1254 - acc: 0.9567 - val_loss: 3.7748 - val_acc: 0.4723\n",
            "Epoch 89/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1064 - acc: 0.9625 - val_loss: 3.7606 - val_acc: 0.4723\n",
            "Epoch 90/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.0888 - acc: 0.9692 - val_loss: 4.0770 - val_acc: 0.4697\n",
            "Epoch 91/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0856 - acc: 0.9700 - val_loss: 4.3005 - val_acc: 0.4578\n",
            "Epoch 92/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1064 - acc: 0.9626 - val_loss: 3.8972 - val_acc: 0.4643\n",
            "Epoch 93/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0932 - acc: 0.9673 - val_loss: 3.9820 - val_acc: 0.4718\n",
            "Epoch 94/100\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 0.0790 - acc: 0.9724 - val_loss: 4.1447 - val_acc: 0.4660\n",
            "Epoch 95/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0825 - acc: 0.9713 - val_loss: 4.1622 - val_acc: 0.4754\n",
            "Epoch 96/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0830 - acc: 0.9699 - val_loss: 4.2234 - val_acc: 0.4664\n",
            "Epoch 97/100\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.1358 - acc: 0.9526 - val_loss: 3.5499 - val_acc: 0.4637\n",
            "Epoch 98/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1013 - acc: 0.9649 - val_loss: 4.0562 - val_acc: 0.4696\n",
            "Epoch 99/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.0699 - acc: 0.9760 - val_loss: 4.1085 - val_acc: 0.4674\n",
            "Epoch 100/100\n",
            "196/196 [==============================] - 15s 77ms/step - loss: 0.1477 - acc: 0.9512 - val_loss: 3.6443 - val_acc: 0.4680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJetCrTFr_S"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3HN95nRFs-U",
        "outputId": "3c04ee31-b656-4364-bf7f-4bf0453ed511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.legend(['train', 'test'])\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9CWqhaEFtNA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}