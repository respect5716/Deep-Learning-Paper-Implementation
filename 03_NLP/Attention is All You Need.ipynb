{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention is All You Need",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "154L-hze1NFX6APB3dVwB15Q07FYj5hwK",
      "authorship_tag": "ABX9TyOTP17qFT7YyKp5jzUZUrkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/respect5716/Deep-Learning-Paper-Implementation/blob/master/03_NLP/Attention%20is%20All%20You%20Need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC",
        "colab_type": "text"
      },
      "source": [
        "# Attention is All You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS",
        "colab_type": "text"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM",
        "colab_type": "text"
      },
      "source": [
        "### Info\n",
        "* TItle : Attention is All You Need\n",
        "* Author : Ashish Vaswani et al.\n",
        "* Publication : [link](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvL7j-dcEHBW",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "* Self Attention을 통해 RNN과 CNN의 한계 극복"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTR7QXvpDz_j",
        "colab_type": "text"
      },
      "source": [
        "### Differences\n",
        "* Task : Translation -> Summarization\n",
        "    * dataset : BBC News Summary, [link](https://www.kaggle.com/pariza/bbc-news-summary?)\n",
        "* Layers : 12 -> 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HVy-eYPLlUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082xq0dJnCDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw-eONQNEZIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "7fae07ed-bfcc-4aba-8d15-b52d73df62cd"
      },
      "source": [
        "# GPU Setting\n",
        "!nvidia-smi\n",
        "\n",
        "print(f'tensorflow version : {tf.__version__}')\n",
        "print(f'available GPU list : {tf.config.list_physical_devices(\"GPU\")}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 16 14:13:40 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "tensorflow version : 2.3.0\n",
            "available GPU list : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'base_dir' : '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "    'seq_len' : 300,\n",
        "    'num_layer' : 4,\n",
        "    'num_head' : 4,\n",
        "    'ffn_dim' : 128,\n",
        "    'model_dim' : 128,\n",
        "    'drop_rate' : 0.2,\n",
        "    'batch_size' : 32,\n",
        "    'epoch_size' : 100\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL1B6fhqqGMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = os.path.join(CONFIG['base_dir'], 'data/bbc_news_summary.zip')\n",
        "!unzip \"{data_path}\" -d /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiSHR3kJrWPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_dir = 'data/News Articles'\n",
        "categories = [i for i in os.listdir(article_dir)]\n",
        "files = [[os.path.join(i,j) for j in os.listdir(os.path.join(article_dir, i))] for i in categories]\n",
        "files = list(itertools.chain(*files))\n",
        "random.shuffle(files)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWfBD0k7thP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_ratio = 0.2\n",
        "split_idx = int(len(files) * split_ratio)\n",
        "train_files = files[split_idx:]\n",
        "test_files = files[:split_idx]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNYh32a_tg-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, files: list, tokenizer: BertTokenizer, mode: str):\n",
        "        self.files = pd.Series(files)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mode = mode\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return np.ceil(len(self.files) / CONFIG['batch_size']).astype(np.int32)\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        self.idx = 0\n",
        "        if self.mode == 'train':\n",
        "            self.indices = np.random.permutation(len(self.files))\n",
        "        else:\n",
        "            self.indices = np.arange(len(self.files))\n",
        "\n",
        "    @staticmethod\n",
        "    def load_text(file_name, dir_name):\n",
        "        with open(os.path.join('data', dir_name, file_name), 'r') as f:\n",
        "            try:\n",
        "                text = f.read()\n",
        "            except:\n",
        "                text = ''\n",
        "        return text\n",
        "    \n",
        "    def encode(self, text, method):\n",
        "        if method == 'encoder':\n",
        "            encoded = self.tokenizer.encode(text, add_special_tokens=False, max_length=CONFIG['seq_len'], truncation=True)\n",
        "        if method == 'decoder_inputs':\n",
        "            encoded = self.tokenizer.encode(text, add_special_tokens=False, max_length=CONFIG['seq_len']-1, truncation=True)\n",
        "            encoded = [self.tokenizer.bos_token_id] + encoded + [self.tokenizer.eos_token_id]\n",
        "        if method == 'decoder_outputs':\n",
        "            encoded = self.tokenizer.encode(text, add_special_tokens=False, max_length=CONFIG['seq_len'], truncation=True)\n",
        "            encoded = encoded + [self.tokenizer.eos_token_id]\n",
        "        \n",
        "        encoded = encoded[:CONFIG['seq_len']]\n",
        "        encoded += [self.tokenizer.pad_token_id for _ in range(CONFIG['seq_len'] - len(encoded))]\n",
        "        return encoded\n",
        "\n",
        "    @classmethod\n",
        "    def decode(cls, encoded):\n",
        "        return tokenizer.decode(encoded)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indices[CONFIG['batch_size']*idx : CONFIG['batch_size']*(idx+1)]\n",
        "        batch_files =  self.files[batch_idx]\n",
        "        encoder_inputs = [self.load_text(i, 'News Articles') for i in batch_files]\n",
        "        decoder_inputs = [self.load_text(i, 'Summaries') for i in batch_files]\n",
        "\n",
        "        invalid_idx = [i for i,j in enumerate(encoder_inputs) if len(j) == 0] + [i for i,j in enumerate(decoder_inputs) if len(j) == 0]\n",
        "        encoder_inputs = [j for i,j in enumerate(encoder_inputs) if i not in invalid_idx]\n",
        "        decoder_inputs = [j for i,j in enumerate(decoder_inputs) if i not in invalid_idx]\n",
        "\n",
        "        encoder_inputs = np.array([self.encode(i, 'encoder') for i in encoder_inputs])\n",
        "        decoder_outputs = np.array([self.encode(i, 'decoder_outputs') for i in decoder_inputs])\n",
        "        decoder_inputs = np.array([self.encode(i, 'decoder_inputs') for i in decoder_inputs])\n",
        "        return (encoder_inputs, decoder_inputs), decoder_outputs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MalZNVZwFqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "_ = tokenizer.add_special_tokens({'bos_token':\"[BOS]\", 'eos_token':'[EOS]'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEgz4EK9Wn1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = Dataloader(train_files, tokenizer, 'train')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esHEf2TqvhQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = train_loader.__getitem__(0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzNeIQZLvmGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "548cd910-cc01-4351-f972-31affa2508a4"
      },
      "source": [
        "x[0].shape, x[1].shape, y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 300), (32, 300), (32, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MaskLayer, self).__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        padding_mask = tf.cast(tf.math.equal(x, 0), tf.float32)[:, None, None, :]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((x.shape[-1], x.shape[-1])), -1, 0)\n",
        "        return padding_mask, look_ahead_mask\n",
        "\n",
        "class EmbeddingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, seq_len, model_dim):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, model_dim)\n",
        "        self.position_embedding = tf.keras.layers.Embedding(seq_len, model_dim)\n",
        "        self.pos = tf.range(0, seq_len)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.token_embedding(x)\n",
        "        pos = self.position_embedding(self.pos)\n",
        "        x += pos\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, num_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_head = num_head\n",
        "        self.projection_dim = self.model_dim // self.num_head\n",
        "        assert self.model_dim % self.num_head == 0\n",
        "\n",
        "        self.qw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.kw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.vw = tf.keras.layers.Dense(self.model_dim)\n",
        "        self.w = tf.keras.layers.Dense(self.model_dim)\n",
        "    \n",
        "    def attention(self, q, k ,v, mask):\n",
        "        dim = tf.cast(tf.shape(q)[-1], tf.float32)\n",
        "        score = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_score = score / tf.math.sqrt(dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_score += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_score)\n",
        "        attention_outputs = tf.matmul(attention_weights, v)\n",
        "        return attention_outputs, attention_weights\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_head, self.projection_dim))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return x\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        x = tf.reshape(x, (batch_size, -1, self.model_dim))\n",
        "        return x\n",
        "    \n",
        "    def call(self, q, k, v, mask):\n",
        "        q, k, v = self.qw(q), self.kw(k), self.vw(v)\n",
        "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "        outputs, weights = self.attention(q, k, v, mask)\n",
        "        outputs = self.combine_heads(outputs)\n",
        "        outputs = self.w(outputs)\n",
        "        return outputs\n",
        "\n",
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, ffn_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(ffn_dim)\n",
        "        self.dense2 = tf.keras.layers.Dense(model_dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def gelu(x):\n",
        "        cdf = 0.5 * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n",
        "        return x * cdf\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_head, model_dim, ffn_dim, drop_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(model_dim, num_head)\n",
        "        self.ffn = FeedForwardNetwork(model_dim, ffn_dim)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, enc, training, padding_mask=None):\n",
        "        out1 = self.mha(enc, enc, enc, padding_mask)\n",
        "        out1 = self.dropout1(out1, training=training)\n",
        "        out1 = self.layernorm1(enc + out1)\n",
        "        out2 = self.ffn(out1)\n",
        "        out2 = self.dropout2(out2, training=training)\n",
        "        out2 = self.layernorm2(out1 + out2)\n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_head, model_dim, ffn_dim, drop_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(model_dim, num_head)\n",
        "        self.mha2 = MultiHeadAttention(model_dim, num_head)\n",
        "        self.ffn = FeedForwardNetwork(model_dim, ffn_dim)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, enc, dec, training, look_ahead_mask=None, padding_mask=None):\n",
        "        out1 = self.mha1(dec, dec, dec, look_ahead_mask)\n",
        "        out1 = self.dropout1(out1, training=training)\n",
        "        out1 = self.layernorm1(dec + out1)\n",
        "        out2 = self.mha2(out1, enc, enc, padding_mask)\n",
        "        out2 = self.dropout2(out2, training=training)\n",
        "        out2 = self.layernorm2(out1 + out2)\n",
        "        out3 = self.ffn(out2)\n",
        "        out3 = self.dropout3(out3, training=training)\n",
        "        out3 = self.layernorm2(out2 + out3)\n",
        "        return out3\n",
        "\n",
        "class OutputLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, x):\n",
        "        outputs = self.dense(x)\n",
        "        return outputs\n",
        "\n",
        "class Network(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Network, self).__init__()\n",
        "        self.mask_layer = MaskLayer()\n",
        "        self.embedding = EmbeddingLayer(vocab_size, CONFIG['seq_len'], CONFIG['model_dim'])\n",
        "        self.encoders = [EncoderLayer(CONFIG['num_head'], CONFIG['model_dim'], CONFIG['ffn_dim'], CONFIG['drop_rate']) for _ in range(CONFIG['num_layer'])]\n",
        "        self.decoders = [DecoderLayer(CONFIG['num_head'], CONFIG['model_dim'], CONFIG['ffn_dim'], CONFIG['drop_rate']) for _ in range(CONFIG['num_layer'])]\n",
        "        self.output_layer = OutputLayer(vocab_size)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        encoder_inputs, decoder_inputs = x\n",
        "        padding_mask, _ = self.mask_layer(encoder_inputs)\n",
        "        _, look_ahead_mask = self.mask_layer(decoder_inputs)\n",
        "\n",
        "        enc = self.embedding(encoder_inputs)\n",
        "        dec = self.embedding(decoder_inputs)\n",
        "\n",
        "        for i in range(CONFIG['num_layer']):\n",
        "            enc = self.encoders[i](enc, padding_mask=padding_mask, training=training)\n",
        "            dec = self.decoders[i](enc, dec, padding_mask=padding_mask, look_ahead_mask=look_ahead_mask, training=training)\n",
        "        \n",
        "        outputs = self.output_layer(dec)\n",
        "        return outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7_PUNrFpCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0bf4ef62-80c9-4935-e76f-117c4c9dae37"
      },
      "source": [
        "network = Network(tokenizer.vocab_size+2)\n",
        "network.optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "pred = network(x)\n",
        "pred.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 300, 30524])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78E3civTbsej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(network, x, y):\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = network(x, training=True)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, pred)\n",
        "\n",
        "    gradients = g.gradient(loss, network.trainable_variables)\n",
        "    network.optimizer.apply_gradients(zip(gradients, network.trainable_variables))  \n",
        "    return tf.reduce_mean(loss) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q57y3u2c576",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f01a7a3-a64e-4de8-a64f-a61d83962f2a"
      },
      "source": [
        "for ep in range(CONFIG['epoch_size']):\n",
        "    for x, y in train_loader:\n",
        "        loss = train_step(network, x, y)\n",
        "    print(f'EP : {str(ep).zfill(3)} | Loss : {loss.numpy():.3f}')\n",
        "\n",
        "print(f'EP : {str(ep+1).zfill(3)} | Loss : {loss.numpy():.3f}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP : 000 | Loss : 4.811\n",
            "EP : 001 | Loss : 4.344\n",
            "EP : 002 | Loss : 4.309\n",
            "EP : 003 | Loss : 4.299\n",
            "EP : 004 | Loss : 4.287\n",
            "EP : 005 | Loss : 4.276\n",
            "EP : 006 | Loss : 4.271\n",
            "EP : 007 | Loss : 4.187\n",
            "EP : 008 | Loss : 4.116\n",
            "EP : 009 | Loss : 4.067\n",
            "EP : 010 | Loss : 3.999\n",
            "EP : 011 | Loss : 3.947\n",
            "EP : 012 | Loss : 3.917\n",
            "EP : 013 | Loss : 3.881\n",
            "EP : 014 | Loss : 3.828\n",
            "EP : 015 | Loss : 3.788\n",
            "EP : 016 | Loss : 3.758\n",
            "EP : 017 | Loss : 3.743\n",
            "EP : 018 | Loss : 3.664\n",
            "EP : 019 | Loss : 3.580\n",
            "EP : 020 | Loss : 3.504\n",
            "EP : 021 | Loss : 3.455\n",
            "EP : 022 | Loss : 3.374\n",
            "EP : 023 | Loss : 3.315\n",
            "EP : 024 | Loss : 3.280\n",
            "EP : 025 | Loss : 3.229\n",
            "EP : 026 | Loss : 3.130\n",
            "EP : 027 | Loss : 3.036\n",
            "EP : 028 | Loss : 2.963\n",
            "EP : 029 | Loss : 2.870\n",
            "EP : 030 | Loss : 2.809\n",
            "EP : 031 | Loss : 2.716\n",
            "EP : 032 | Loss : 2.632\n",
            "EP : 033 | Loss : 2.536\n",
            "EP : 034 | Loss : 2.449\n",
            "EP : 035 | Loss : 2.336\n",
            "EP : 036 | Loss : 2.234\n",
            "EP : 037 | Loss : 2.141\n",
            "EP : 038 | Loss : 2.103\n",
            "EP : 039 | Loss : 2.008\n",
            "EP : 040 | Loss : 1.929\n",
            "EP : 041 | Loss : 1.841\n",
            "EP : 042 | Loss : 1.745\n",
            "EP : 043 | Loss : 1.694\n",
            "EP : 044 | Loss : 1.583\n",
            "EP : 045 | Loss : 1.520\n",
            "EP : 046 | Loss : 1.446\n",
            "EP : 047 | Loss : 1.391\n",
            "EP : 048 | Loss : 1.336\n",
            "EP : 049 | Loss : 1.305\n",
            "EP : 050 | Loss : 1.240\n",
            "EP : 051 | Loss : 1.191\n",
            "EP : 052 | Loss : 1.113\n",
            "EP : 053 | Loss : 1.062\n",
            "EP : 054 | Loss : 1.023\n",
            "EP : 055 | Loss : 0.982\n",
            "EP : 056 | Loss : 0.932\n",
            "EP : 057 | Loss : 0.879\n",
            "EP : 058 | Loss : 0.830\n",
            "EP : 059 | Loss : 0.807\n",
            "EP : 060 | Loss : 0.775\n",
            "EP : 061 | Loss : 0.765\n",
            "EP : 062 | Loss : 0.713\n",
            "EP : 063 | Loss : 0.688\n",
            "EP : 064 | Loss : 0.665\n",
            "EP : 065 | Loss : 0.636\n",
            "EP : 066 | Loss : 0.582\n",
            "EP : 067 | Loss : 0.583\n",
            "EP : 068 | Loss : 0.560\n",
            "EP : 069 | Loss : 0.541\n",
            "EP : 070 | Loss : 0.523\n",
            "EP : 071 | Loss : 0.506\n",
            "EP : 072 | Loss : 0.492\n",
            "EP : 073 | Loss : 0.457\n",
            "EP : 074 | Loss : 0.462\n",
            "EP : 075 | Loss : 0.414\n",
            "EP : 076 | Loss : 0.402\n",
            "EP : 077 | Loss : 0.400\n",
            "EP : 078 | Loss : 0.383\n",
            "EP : 079 | Loss : 0.361\n",
            "EP : 080 | Loss : 0.354\n",
            "EP : 081 | Loss : 0.357\n",
            "EP : 082 | Loss : 0.339\n",
            "EP : 083 | Loss : 0.312\n",
            "EP : 084 | Loss : 0.307\n",
            "EP : 085 | Loss : 0.292\n",
            "EP : 086 | Loss : 0.299\n",
            "EP : 087 | Loss : 0.267\n",
            "EP : 088 | Loss : 0.262\n",
            "EP : 089 | Loss : 0.251\n",
            "EP : 090 | Loss : 0.273\n",
            "EP : 091 | Loss : 0.245\n",
            "EP : 092 | Loss : 0.230\n",
            "EP : 093 | Loss : 0.226\n",
            "EP : 094 | Loss : 0.228\n",
            "EP : 095 | Loss : 0.221\n",
            "EP : 096 | Loss : 0.218\n",
            "EP : 097 | Loss : 0.215\n",
            "EP : 098 | Loss : 0.219\n",
            "EP : 099 | Loss : 0.215\n",
            "EP : 100 | Loss : 0.215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJetCrTFr_S",
        "colab_type": "text"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh_A8ntfkkdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_greedy(network, encoder_inputs):\n",
        "    encoder_inputs = np.array(encoder_inputs)[None, :]\n",
        "    decoder_inputs = np.ones((1, CONFIG['seq_len'])) * tokenizer.pad_token_id\n",
        "    decoder_inputs[0][0] = tokenizer.bos_token_id\n",
        "\n",
        "    for i in range(1, CONFIG['seq_len']):\n",
        "        pred = network((encoder_inputs, decoder_inputs), training=False)\n",
        "        pred = tf.random.categorical(tf.math.log(pred[0][i:i+1] + 1e-5), 1)[0][0]\n",
        "        decoder_inputs[0][i] = pred\n",
        "        if pred == tokenizer.eos_token_id:\n",
        "            break\n",
        "    \n",
        "    return decoder_inputs[0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr2jqSiFmizS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = Dataloader(test_files, tokenizer, 'test')\n",
        "x, y = test_loader.__getitem__(0)\n",
        "source = x[0][0]\n",
        "summarized = predict_greedy(network, source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOOoCbdlov0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Source')\n",
        "print(tokenizer.decode(source))\n",
        "print('\\n')\n",
        "print('Summarized')\n",
        "print(tokenizer.decode(summarized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lEGkNeXkXJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}